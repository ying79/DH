#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Aozora Bunko downloader (balanced random + sidecar)
- éš¨æ©Ÿä¸‹è¼‰ N æœ¬ï¼ˆæ¯ä½ä½œè€…è‡³å¤š per_author æœ¬ï¼Œé è¨­ 1ï¼‰
- ä»¥ä½œè€…é—œéµå­—ä¸‹è¼‰
- ä»¥ä½œè€… + æ¨™é¡Œé—œéµå­—ä¸‹è¼‰
- ç›´æ¥æŒ‡å®šä½œå“å¡é  URL ä¸‹è¼‰
- å„ªå…ˆæŠ“ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«(ãƒ«ãƒ“ã‚ã‚Š)ã€zipï¼Œé€€å›ä»»ä½• zip
- è‡ªå‹•è™•ç† Shift-JIS/CP932ï¼Œè¼¸å‡º UTF-8 .txt
- å¦å­˜ *.meta.jsonï¼ˆtitle/author/card_urlï¼‰æ–¹ä¾¿ RAG é¡¯ç¤º

ç”¨æ³•ï¼š
  python aozora_downloader.py 3
  python aozora_downloader.py 5 --author å¤ç›®æ¼±çŸ³
  python aozora_downloader.py 1 --author å¤ç›®æ¼±çŸ³ --title å¾è¼©ã¯çŒ«
  python aozora_downloader.py 1 --card https://www.aozora.gr.jp/cards/000148/cardXXXX.html
  python aozora_downloader.py 50 --per-author 1
"""

import argparse
import io
import os
import random
import re
import unicodedata
import zipfile
from pathlib import Path
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

INDEX_ALL = "https://www.aozora.gr.jp/index_pages/person_all.html"
INDEX_FALLBACKS = [
    "https://www.aozora.gr.jp/index_pages/person_a.html",
    "https://www.aozora.gr.jp/index_pages/person_ka.html",
    "https://www.aozora.gr.jp/index_pages/person_sa.html",
    "https://www.aozora.gr.jp/index_pages/person_ta.html",
    "https://www.aozora.gr.jp/index_pages/person_na.html",
    "https://www.aozora.gr.jp/index_pages/person_ha.html",
    "https://www.aozora.gr.jp/index_pages/person_ma.html",
    "https://www.aozora.gr.jp/index_pages/person_ya.html",
    "https://www.aozora.gr.jp/index_pages/person_ra.html",
    "https://www.aozora.gr.jp/index_pages/person_wa.html",
]
UA = "Mozilla/5.0 (Macintosh; Intel Mac OS X) AozoraScraper/1.2"

def _get_soup(url: str, session: requests.Session | None = None) -> BeautifulSoup:
    sess = session or requests.Session()
    r = sess.get(url, timeout=25, headers={"User-Agent": UA})
    ct = r.headers.get("Content-Type", "").lower()
    head = r.content[:800].lower()
    if "shift_jis" in ct or b"shift_jis" in head or b"x-sjis" in head:
        r.encoding = "cp932"
    else:
        r.encoding = r.apparent_encoding or "utf-8"
    return BeautifulSoup(r.text, "html.parser")

_norm_space = re.compile(r"[\s\u3000ãƒ»ï½¥]+")
def jnorm(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = _norm_space.sub("", s)
    return s

def sanitize_filename(s: str) -> str:
    s = unicodedata.normalize("NFKC", s).strip()
    s = re.sub(r"[\\/:*?\"<>|]", "_", s)
    s = re.sub(r"\s+", "_", s)
    return s or "untitled"

def get_all_author_pages() -> list[dict]:
    sess = requests.Session()
    authors: list[dict] = []
    soup = _get_soup(INDEX_ALL, session=sess)
    links = soup.select("ol li a[href^='person']")
    if not links:
        print("â„¹ï¸ person_all ç„¡å…§å®¹ï¼Œæ”¹ç”¨å¾Œå‚™å‡åå­é æ¸…å–®ã€‚")
        for u in INDEX_FALLBACKS:
            s2 = _get_soup(u, session=sess)
            links += s2.select("ol li a[href^='person']")
    for a in links:
        href = a.get("href")
        name = a.get_text(strip=True)
        if not href or not name:
            continue
        authors.append({"name": name, "url": urljoin(INDEX_ALL, href)})
    # å»é‡
    seen, dedup = set(), []
    for x in authors:
        if x["url"] in seen:
            continue
        seen.add(x["url"])
        dedup.append(x)
    return dedup

def find_authors_by_keyword(keyword: str) -> list[dict]:
    key = jnorm(keyword)
    return [row for row in get_all_author_pages() if key in jnorm(row["name"])]

def get_author_card_pages(author_page_url: str) -> list[dict]:
    soup = _get_soup(author_page_url)
    out: list[dict] = []
    # å…¬é–‹ä¸­ã®ä½œå“
    anchor = soup.select_one("a[name='sakuhin_list_1']")
    if anchor:
        ol = anchor.find_next("ol")
        if ol:
            for a in ol.select("a[href*='cards/'][href*='card']"):
                out.append({"title": a.get_text(strip=True), "url": urljoin(author_page_url, a.get("href"))})
    if not out:
        for a in soup.select("a[href*='cards/'][href*='card']"):
            out.append({"title": a.get_text(strip=True), "url": urljoin(author_page_url, a.get("href"))})
    # å»é‡
    seen, dedup = set(), []
    for x in out:
        if x["url"] in seen:
            continue
        seen.add(x["url"])
        dedup.append(x)
    return dedup

def get_card_page_title(card_url: str) -> str:
    soup = _get_soup(card_url)
    text = soup.get_text("\n", strip=True)
    m = re.search(r"ä½œ[ã€€\s]*å“[ã€€\s]*å[:ï¼š]\s*([^\n\r]+)", text)
    if m:
        return m.group(1).strip()
    for tag in ["h2", "h3"]:
        h = soup.find(tag)
        if h and h.get_text(strip=True) and not re.search(r"å›³æ›¸ã‚«ãƒ¼ãƒ‰", h.get_text()):
            return h.get_text(strip=True)
    t = soup.find("title")
    if t:
        return re.sub(r"å›³æ›¸ã‚«ãƒ¼ãƒ‰[:ï¼š]?\s*", "", t.get_text(strip=True))
    return "work"

def get_card_page_author(card_url: str) -> str:
    soup = _get_soup(card_url)
    a = soup.select_one("a[href*='index_pages/person']")
    if a and a.get_text(strip=True):
        return a.get_text(strip=True)
    lab = soup.find(string=re.compile(r"è‘—è€…å|ä½œè€…å"))
    if lab and lab.parent:
        txt = lab.parent.get_text(" ", strip=True)
        m = re.search(r"(è‘—è€…å|ä½œè€…å)[:ï¼š]\s*([^\sã€€]+)", txt)
        if m:
            return m.group(2)
    return ""

def find_zip_from_card(card_url: str) -> tuple[str | None, str]:
    soup = _get_soup(card_url)
    page_title = get_card_page_title(card_url)
    cand = soup.select("a[href$='.zip']")
    ruby_first, any_zip = [], []
    for a in cand:
        href = a.get("href", "")
        text = a.get_text(" ", strip=True)
        if re.search(r"ruby|ãƒ«ãƒ“", href, re.I) or re.search(r"ãƒ«ãƒ“", text):
            ruby_first.append(urljoin(card_url, href))
        else:
            any_zip.append(urljoin(card_url, href))
    if ruby_first:
        return ruby_first[0], page_title
    if any_zip:
        return any_zip[0], page_title
    return None, page_title

# æ›¿æ›åŸæœ¬çš„ download_and_extract_textï¼ŒåŠ ä¸Šç°¡å–®é‡è©¦èˆ‡é€€é¿
def download_and_extract_text(zip_url: str, out_dir: str, base_title: str, max_retries: int = 3) -> list[str]:
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    last_err = None
    for attempt in range(max_retries):
        try:
            r = requests.get(zip_url, timeout=40, headers={"User-Agent": UA})
            r.raise_for_status()
            saved = []
            with zipfile.ZipFile(io.BytesIO(r.content)) as zf:
                for info in zf.infolist():
                    if not info.filename.lower().endswith(".txt"):
                        continue
                    data = zf.read(info.filename)
                    try:
                        text = data.decode("cp932")
                    except UnicodeDecodeError:
                        text = data.decode("shift_jis", errors="ignore")
                    inner_name = os.path.basename(info.filename)
                    stem = os.path.splitext(inner_name)[0]
                    fn = f"{sanitize_filename(stem)}.txt"
                    if len(stem) < 2 or re.fullmatch(r"\d+_ruby_\d+", stem):
                        fn = f"{sanitize_filename(base_title)}.txt"
                    out_path = Path(out_dir) / fn
                    out_path.write_text(text, encoding="utf-8")
                    saved.append(str(out_path))
            return saved
        except Exception as e:
            last_err = e
            # æŒ‡æ•¸é€€é¿ï¼ˆ0.8~1.4 éš¨æ©ŸæŠ–å‹•ï¼‰
            import time, random
            time.sleep((0.8 + 0.6 * random.random()) * (2 ** attempt))
    # é‡è©¦ç”¨ç›¡ï¼Œä¸Ÿå›è®“ä¸Šå±¤æ‰“å°ã€Œâš ï¸ å¤±æ•—ã€
    raise last_err


def write_sidecar(txt_path: Path, title: str, author: str, card_url: str):
    meta = {"title": title, "author": author, "card_url": card_url}
    txt_path.with_suffix(".meta.json").write_text(
        __import__("json").dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
    )

def download_by_card(card_url: str, out_dir: str) -> int:
    zip_url, title = find_zip_from_card(card_url)
    if not zip_url:
        print("âŒ é€™å€‹å¡é æ²’æœ‰å¯ä¸‹è¼‰çš„ zipã€‚")
        return 0
    print("ğŸ”— Fetching", card_url)
    try:
        saved = download_and_extract_text(zip_url, out_dir, base_title=title)
        if saved:
            author = get_card_page_author(card_url) or ""
            for p in saved:
                write_sidecar(Path(p), title, author, card_url)
            print(f"âœ… Saved: {os.path.basename(saved[0])}")
            return 1
    except Exception as e:
        print("âš ï¸ å¤±æ•—ï¼š", e)
    return 0

def download_by_author(keyword: str, count: int, out_dir: str, title_kw: str | None = None) -> int:
    matches = find_authors_by_keyword(keyword)
    if not matches:
        print(f"âŒ æ‰¾ä¸åˆ°ç¬¦åˆä½œè€…é—œéµå­—ï¼š{keyword}")
        return 0
    author = matches[0]
    print(f"ğŸ‘¤ å‘½ä¸­ä½œè€…ï¼š{author['name']} â†’ {author['url']}")

    cards = get_author_card_pages(author["url"])
    if not cards:
        print("âŒ æ‰¾ä¸åˆ°å…¬é–‹ä¸­çš„ä½œå“ã€‚")
        return 0
    if title_kw:
        key = jnorm(title_kw)
        filtered = [c for c in cards if key in jnorm(c["title"]) or key in jnorm(get_card_page_title(c["url"]))]
        if not filtered:
            print(f"âŒ åœ¨æ­¤ä½œè€…ä¸‹æ‰¾ä¸åˆ°æ¨™é¡ŒåŒ…å«ã€Œ{title_kw}ã€çš„ä½œå“ã€‚")
            return 0
        cards = filtered

    downloaded = 0
    for c in cards:
        if downloaded >= count:
            break
        zip_url, title = find_zip_from_card(c["url"])
        if not zip_url:
            continue
        print("ğŸ”— Fetching", c["url"])
        try:
            saved = download_and_extract_text(zip_url, out_dir, base_title=title)
            if saved:
                for p in saved:
                    write_sidecar(Path(p), title, author["name"], c["url"])
                print(f"âœ… Saved: {os.path.basename(saved[0])}")
                downloaded += 1
        except Exception as e:
            print("âš ï¸ å¤±æ•—ï¼š", e)
    print(f"ğŸ‰ å®Œæˆï¼Œå…±ä¸‹è¼‰ {downloaded} æœ¬ã€‚")
    return downloaded

def download_random(count: int, out_dir: str, per_author: int = 1) -> int:
    """éš¨æ©Ÿå¤šä½œè€…ä¸‹è¼‰ï¼šæ¯ä½ä½œè€…è‡³å¤š per_author æœ¬ï¼ˆé è¨­ 1ï¼‰"""
    authors = get_all_author_pages()
    if not authors:
        print("âŒ å–å¾—ä½œè€…æ¸…å–®å¤±æ•—ã€‚")
        return 0
    random.shuffle(authors)
    downloaded = 0
    for a in authors:
        if downloaded >= count:
            break
        cards = get_author_card_pages(a["url"])
        if not cards:
            continue
        random.shuffle(cards)
        taken = 0
        for c in cards:
            if downloaded >= count or taken >= max(1, per_author):
                break
            zip_url, title = find_zip_from_card(c["url"])
            if not zip_url:
                continue
            print(f"ğŸ‘¤ {a['name']} â†’ {c['url']}")
            try:
                saved = download_and_extract_text(zip_url, out_dir, base_title=title)
                if saved:
                    for p in saved:
                        write_sidecar(Path(p), title, a["name"], c["url"])
                    print(f"âœ… Saved: {os.path.basename(saved[0])}")
                    downloaded += 1
                    taken += 1
            except Exception as e:
                print("âš ï¸ å¤±æ•—ï¼š", e)
    print(f"ğŸ‰ å®Œæˆï¼Œå…±ä¸‹è¼‰ {downloaded} æœ¬ã€‚")
    return downloaded

def main():
    ap = argparse.ArgumentParser(description="Aozora Bunko downloader")
    ap.add_argument("count", type=int, help="è¦ä¸‹è¼‰çš„æœ¬æ•¸")
    ap.add_argument("--author", type=str, default=None, help="ä½œè€…é—œéµå­—ï¼ˆä¾‹ï¼šå¤ç›®æ¼±çŸ³ï¼‰")
    ap.add_argument("--title", type=str, default=None, help="ï¼ˆæ­é… --authorï¼‰ä½œå“æ¨™é¡Œé—œéµå­—")
    ap.add_argument("--card", type=str, default=None, help="ç›´æ¥æŒ‡å®šä½œå“å¡é  URL")
    ap.add_argument("--out", type=str, default="data", help="è¼¸å‡ºè³‡æ–™å¤¾ï¼ˆé è¨­ dataï¼‰")
    ap.add_argument("--per-author", type=int, default=1, help="éš¨æ©Ÿæ¨¡å¼ï¼šæ¯ä½ä½œè€…æœ€å¤šå¹¾æœ¬ï¼ˆé è¨­ 1ï¼‰")
    args = ap.parse_args()

    Path(args.out).mkdir(parents=True, exist_ok=True)

    if args.card:
        # ç›´æ¥å¡é æ¨¡å¼ï¼šè‡³å¤šä¸‹è¼‰ 1 æœ¬
        return download_by_card(args.card, args.out)

    if args.author:
        return download_by_author(args.author, args.count, args.out, title_kw=args.title)

    return download_random(args.count, args.out, per_author=args.per_author)

if __name__ == "__main__":
    main()
